---
title: "Capstone Report"
author: "Evan Huang"
date: "2023-01-14"
output: pdf_document
---
<!-- Setting up and loading data generated by script.-->
```{r setup, echo = FALSE, include = FALSE}
if (!require("knitr")) install.packages("knitr")
library(knitr)
load("./Data/Exploration.rda")
load("./Data/Capstone.rda")
opts_chunk$set(echo = T)
```
<!-- an introduction/overview/executive summary section that 
describes the dataset and summarizes the goal of the project 
and key steps that were performed-->
# Introduction
For this data analysis project I built a recommendation system for 
the movielens 10M dataset. The movielens dataset has movie ratings by users with 
the rating the user gave the movie, the timestamp of the rating, 
and the genres of the movie.

```{r Original Test Set, results = "asis"}
kable(head(edx,5))
```
The data was manipulated to extract features and regularization was used for 
a tree model to accurately predict how users would rate movies in the dataset.

<!-- a methods/analysis section that explains the process and techniques used, 
including data cleaning, data exploration and visualization, insights gained, 
and your modeling approach -->
\newpage
# Methods
The movielens dataset was split into a test (edx) and a train set (validation). 
In order to avoid overfitting, 
the validation set was ignored entirely until the end. 
Both sets were manipulated to create features that were 
not dependent on the dataset they were in, 
which were the release year of the movie (based on title), 
the year the movie was rated (based on timestamp), 
and the genres of the movie (based on genres and genrelist). 
Genres were defined in the README file for the dataset: 
https://files.grouplens.org/datasets/movielens/ml-10m-README.html.

```{r Genres}
genrelist
```

The number of ratings each movie had and each user submitted 
both relied on the specific ratings in the test set, 
so I decided to evaluate those after splitting the edx set into train/test sets.

To help analyze the data, I created a dataset that had the movieId, 
average rating, release year, genres, and number of ratings for each movie.

```{r Movies}
movie_info
```
\newpage
## General Correlations and Number of Genres
The linear correlations between the rating and other features tended 
to be low in both the edx set and the averaged movie rating set.

```{r Correlations}
cors
movie_cors
```

As for genres, the genres did not have an equal number of movies or ratings, 
but none of them were very low.

```{r Genre Counts}
genre_count
movie_genre_count
```

\newpage
## General Plots
The number of ratings a movie received as well as its release year also
had relatively strong correlations with the average rating a movie received. 
The user ratings were not shown in the averaged movie rating set, 
but there seemed to be a smaller correlation with the between 
the number of ratings and the average rating a user gave.

```{r Ratings and Release Plots, echo = F}
movie_ratings_plot
release_plot
user_ratings_plot
```

\newpage
## Genre-Related Plots
Drama and Horror had the strongest correlations while 
Fantasy and Thriller has the lowest correlations out of the genres. 
This can be seen by graphing the movies that are a specific genre against 
those that aren't. The Drama and Horror plots had much less overlap than the
Fantasy and Thriller plots.
```{r Genre Plots, echo = F}
drama_plot
horror_plot
fantasy_plot
thriller_plot
```

## Regularization
I decided to try 12 models, which differed by the genres they used 
(None of them, Drama and Horror, or all of them), if they were
a linear or a tree model, and if they used regularization. I used 3 different 
train/test set pairs derived from the edx set for consistency.

To regularize the movie and user effects, I used the 3 linear models that 
differed by the genres they used to save time and took a weighted average 
RMSE over the 3 train/test set pairs that punished the highest RMSEs 
and rewarded lowest RMSEs. The lambdas I got from the regularization were:

```{r Model Lambdas}
kable(lambdas)
```

<!-- a results section that presents the modeling results and discusses the model performance -->
\newpage
# Results
The results of the 12 models were as follows:

```{r Model RMSEs}
kable(rmses)
```
As expected, the models that used more genres tended to do better than their 
counterparts. The exception were the non-regularized linear models, which were 
all approximately equal. Their regularized counterparts did follow the pattern.
Additionally, the tree models did better than their linear counterparts. 
Although the majority of the regularized models performed better, the tree model 
using all genres performed slightly worse with regularization than not. 
This is likely due to overfitting or the regularization lambdas being derived 
from the linear models and therefore not necessarily being optimized for the 
tree models. Ultimately, I decided to use the regularized tree model that used
all genres with the final validation set. 

```{r Final RMSE}
final_rmse
```
The final RMSE ended up being lower than any
of the previous results, which is likely due to the fact that the train and
test sets used previously while were smaller than the final edx and 
validation set.

<!-- a conclusion section that gives a brief summary of the report, its limitations and future work -->
# Conclusion
The final model that used regularized user/movie effects, a tree model, and 
genres was effective, with its RMSE of being lower than the 0.86490 threshold. 
There are however many possible areas of improvement. The final model is limited 
by hardware and computation time and does not fully utilize xgb's parameters, 
leaving the parameters to their default values. The model is also unable to 
extend to users and movies outside of its dataset. Using neighborhoods to deal 
with movies/users with few ratings could be part of future work.